These contains misc functions useful in somewhat rare cases.

Get human-readable timestamp from internet archive URL.
When viewing an archived page, the IA stores the timestamp in this format:
	https://web.archive.org/web/YYYYMMDDhhmmss/<Website original URL>
	
	YYYYMMDDhhmmss: Each character is a digit of a number. If fewer digits,
	leading zeroes are added.
	YYYY = year
	MM = months
	DD = day
	hh = hour
	mm = minute
	ss = second

	This regular expression replaces the timestamp to readable format:
		Find what: [(?'ArchiveURL'https://web.archive.org/web/(?'Year'\d\d\d\d)(?'Month'\d\d)(?'Day'\d\d)(?'Hours'\d\d)(?'Minutes'\d\d)(?'Seconds'\d\d)/(.*)$)]
		Replace with: [$+{Year}-$+{Month}-$+{Day} $+{Hours}:$+{Minutes}:$+{Seconds} Link: $+{ArchiveURL}]
		Search mode: Regular expression.
		
		
Continuous saving via "decaying" list
	Perhaps if you have thousands of URLs and are submitting each URL containing 500 or less links on there
	all at once. So I provided a strategy checking if the URLs have failed or are waiting to get a reply on it.
	Before even submitting, have a "master list" that CAN have more than 500 (sorted alphabetically, no duplicate lines),
	take each group of 500 and submit them. Once you get the last group. Then as each reply is sent to your inbox, copy all those URLs detected, and do this:
		The reply from the IA:
			Remove the potential "First Archive":
				Find what: [ First Archive$]
				Replace with: [] (nothing)
				Wrap around: checked
				Search mode: Regular expression
				. matches newline: unchecked
			Then, we want to reformat the successful saves so that it starts with the original URL (so that later when
			sorting alphabetically with both sent and reply list combined, you have the sent links and the successfully
			saved link from reply next to one another):
				Find what: [https://web.archive.org/web/[0-9]*/(?'URL'https://([^[:s:]]*))*]
				Replace with: [$+{URL} success]
				Wrap around: checked
				Search mode: Regular expression
				. matches newline: unchecked
					^This will revert the Internet archive URL to an original URL with the word "success" added after only for URLs that
					have truly sucessfully saved.
					
		Ridding out the URLs that were saved:
			Then copy that reformatted list and combine it with the master list. Sort alphabetically ascending (Edit -> Line Operations -> Sort Lines Lexicographically ascending,
			not descending!). Then do this:
				Find what: [^(?'URL'https://([^[:s:]]+))\R\g'URL' success]
				Replace with: [] (nothing)
				Wrap around: checked
				Search mode: Regular expression
					^This will get rid of all URLs that have successfully been saved (confirmed), while ones that only 1 exists
					(either due to redirect, or waiting for a reply), errors out or any "unsure" if the URL have been saved or not
					will still exist to be (re-)saved.
			Feel free to remove duplicate URLs by Edit -> Line Operations -> Remove Consecutive Duplicate lines (this brings all the URLs
			together and removes all but one empty line) afterwards.
			Since errors and whatever on the list are both "not saved" URLs, do this to clean up:
				Find what: [^https://[^[:s:]]* Error!.*$]
				Replace with: [] (nothing)
				Wrap around: checked
				Search mode: Regular expression
	Essentially, the "master list" is a "to save/resave list". If any fails, remains on the list to be tried again, and successfully saved links
	disappear off the list.
	
	Be very careful if the link contains special characters not being percent encoded. This replace mechanism assumes the space character marks the end
	of the URL and so does most online platforms when posting a reply. Also note that the space character in a FTP have two symbols for substitution with
	safer characters:
		[+] (plus sigh) which then becomes %2B
		[ ] (space character) becomes %20
	Which they are interpreted differently.
	

Credits:
	Notepad++ community:
		sasumner ( https://github.com/sasumner ) and xylographe ( https://github.com/xylographe )
			Got help in this thread:
				https://github.com/notepad-plus-plus/notepad-plus-plus/issues/8102
			boost.org's info:
				https://www.boost.org/doc/libs/1_70_0/libs/regex/doc/html/boost_regex/format/perl_format.html
				https://www.boost.org/doc/libs/1_70_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html
		
		for helping me with using whatever is matched into a capture group. This is often useful for moving texts around.